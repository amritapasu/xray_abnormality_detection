{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a711da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.9/site-packages (0.22.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.9/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.9/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.9/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.9/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.9/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.9/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.9/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.9/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.9/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.9/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.9/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.9/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.venv/lib/python3.9/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.9/site-packages (from triton==3.3.0->torch) (53.0.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.9/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba7829a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47132b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7065abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f27c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc66041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504b2d8",
   "metadata": {},
   "source": [
    "# CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddf1163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##MAKE SURE TO CHANGE THIS\n",
    "best_model_path = \"models/nf_d1.pth\"\n",
    "target_col = 'No Finding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14638654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_root_dir, target_columns=None, transform=None,\n",
    "                 save_dir=None, use_saved_images=False):\n",
    "        self.data = dataframe\n",
    "        self.image_root_dir = image_root_dir\n",
    "        self.target_columns = target_columns\n",
    "        self.transform = transform\n",
    "        self.save_dir = save_dir\n",
    "        self.use_saved_images = use_saved_images\n",
    "\n",
    "        if self.save_dir:\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Use index for the saved tensor filename\n",
    "        image_index = row['Unnamed: 0']\n",
    "        saved_image_path = os.path.join(self.save_dir, f\"{image_index}.pt\")\n",
    "\n",
    "        if self.use_saved_images:\n",
    "            if os.path.exists(saved_image_path):\n",
    "                image_tensor = torch.load(saved_image_path)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Saved tensor not found: {saved_image_path}\")\n",
    "        else:\n",
    "            original_image_path = os.path.join(self.image_root_dir, row['Path'])\n",
    "            image = Image.open(original_image_path).convert(\"L\")\n",
    "            image_tensor = self.transform(image) if self.transform else transforms.ToTensor()(image)\n",
    "\n",
    "            if self.save_dir:\n",
    "                torch.save(image_tensor, saved_image_path)\n",
    "\n",
    "        if self.target_columns:\n",
    "            labels = pd.to_numeric(row[self.target_columns], errors='coerce').fillna(0).astype(float).values\n",
    "            labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            return image_tensor, labels\n",
    "\n",
    "        return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b15e5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim = 512, dropout_rate = 0.5):\n",
    "        super(MultiLabelResNet50, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet50\n",
    "        self.base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Modify the fully connected layer for multi-label classification\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(self.base_model.fc.in_features, hidden_dim),  # New intermediate layer. ##512 --> 256\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # Dropout to prevent overfitting ##0.5 --> 0.6\n",
    "            nn.Linear(hidden_dim, num_classes),  # Output layer\n",
    "            nn.Sigmoid()  # Sigmoid for multi-label classification (soften the data)\n",
    "            #nn.Tanh()  #This is between -1 and 1\n",
    "\n",
    "           # nn.Linear(self.base_model.fc.in_features, num_classes),\n",
    "           # nn.Sigmoid()  # Sigmoid activation for multi-label classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c22b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelResNet50_2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiLabelResNet50_2, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet50\n",
    "        self.base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Modify the fully connected layer for multi-label classification\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(self.base_model.fc.in_features, 256),  # New intermediate layer. ##512 --> 256\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),  # Dropout to prevent overfitting ##0.5 --> 0.6\n",
    "            nn.Linear(256, num_classes),  # Output layer\n",
    "            nn.Sigmoid()  # Sigmoid for multi-label classification (soften the data)\n",
    "            #nn.Tanh()  #This is between -1 and 1\n",
    "\n",
    "           # nn.Linear(self.base_model.fc.in_features, num_classes),\n",
    "           # nn.Sigmoid()  # Sigmoid activation for multi-label classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e328c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDenseNet121(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim = 512):\n",
    "        super(MultiLabelDenseNet121, self).__init__()\n",
    "\n",
    "        # Load pre-trained DenseNet-121\n",
    "        self.base_model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Replace the classifier with a custom head\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Linear(self.base_model.classifier.in_features, hidden_dim),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "            nn.Tanh()  # Output values in [-1, 1] for each class\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3889a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root = '/central/groups/CS156b/2025/CodeMonkeys/input_images'\n",
    "image_root_dir = \"input_images/train\"\n",
    "train_save_dir = os.path.join(image_root, 'train')\n",
    "\n",
    "def get_filtered_df(col, num=None):\n",
    "    \n",
    "    #test_save_dir = os.path.join(image_root, 'test')\n",
    "    full_train_df = pd.read_csv('train2023.csv')\n",
    "    #print(len(full_train_df))\n",
    "    #filtered_train_df = full_train_df.iloc[:29692]\n",
    "    if num != None:\n",
    "        full_train_df = full_train_df.iloc[:num]\n",
    "\n",
    "    filtered_train_df = full_train_df.dropna(subset=[col]).copy()\n",
    "    filtered_train_df[col] = (filtered_train_df[col] + 1) / 2\n",
    "    #filtered_train_df = filtered_train_df[filtered_train_df[col] != 0.5]  # Drop rows with 'Pleural_Effusion' == 0\n",
    "    return filtered_train_df\n",
    "\n",
    "filtered_train_df = get_filtered_df(target_col)\n",
    "#print(len(filtered_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339e4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "##DONT GENERALLY WANT THIS\n",
    "label_counts = filtered_train_df[target_col].value_counts()\n",
    "\n",
    "# Get majority class (should be 1.0)\n",
    "majority_class = label_counts.idxmax()\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_df = filtered_train_df[filtered_train_df[target_col] == majority_class]\n",
    "minority_df = filtered_train_df[filtered_train_df[target_col] != majority_class]\n",
    "\n",
    "# Randomly sample 10,000 from the majority class\n",
    "majority_df = majority_df.sample(n=10000, random_state=42)\n",
    "minority_df = minority_df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Combine back\n",
    "balanced_df = pd.concat([majority_df, minority_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "filtered_train_df = balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0238a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "##DONT GENERALLY WANT THIS\n",
    "# # Define desired number of samples per class\n",
    "sample_sizes = {\n",
    "    0.0: 15000,\n",
    "    0.5: 7000,\n",
    "    1.0: 15000\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for label, n_samples in sample_sizes.items():\n",
    "    class_df = filtered_train_df[filtered_train_df[target_col] == label]\n",
    "    # Sample with replacement if not enough data\n",
    "    sampled_df = class_df.sample(n=n_samples, replace=(n_samples > len(class_df)), random_state=42)\n",
    "    dfs.append(sampled_df)\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_df = pd.concat(dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "filtered_train_df = balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e851ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Finding\n",
      "1.0    15000\n",
      "0.0    15000\n",
      "0.5     7000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = filtered_train_df[target_col].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6baeffc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frontal/Lateral\n",
      "Frontal    28873\n",
      "Lateral     8127\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(filtered_train_df[\"Frontal/Lateral\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "140c7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your target columns once\n",
    "target_columns = [target_col]\n",
    "\n",
    "# Step 1: Split the dataframe\n",
    "train_df, val_df = train_test_split(filtered_train_df, test_size=0.15, random_state=42)\n",
    "\n",
    "##MAY WANT TO GET RID OF THESE\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# Step 2: Create training dataset\n",
    "train_dataset = CSVDataset(\n",
    "    dataframe=train_df, \n",
    "    image_root_dir=image_root, \n",
    "    target_columns=target_columns, \n",
    "    save_dir=train_save_dir, \n",
    "    use_saved_images=True\n",
    ")\n",
    "\n",
    "# Step 3: Create validation dataset\n",
    "val_dataset = CSVDataset(\n",
    "    dataframe=val_df, \n",
    "    image_root_dir=image_root, \n",
    "    target_columns=target_columns, \n",
    "    save_dir=train_save_dir, \n",
    "    use_saved_images=True\n",
    ")\n",
    "\n",
    "# Step 4: Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) ##64 --> 32\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb31f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is for oversampling\n",
    "fracture_labels = train_df[target_col].values\n",
    "label_map = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "mapped_labels = np.array([label_map[float(lbl)] for lbl in fracture_labels])\n",
    "\n",
    "class_counts = np.bincount(mapped_labels)\n",
    "weights = 1. / (class_counts + 1e-6)\n",
    "sample_weights = torch.tensor(weights[mapped_labels], dtype=torch.float)\n",
    "\n",
    "sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ee0454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_base_layers_dn(model):\n",
    "    \"\"\"\n",
    "    Freeze initial layers of DenseNet-121 up to denseblock3.\n",
    "    \"\"\"\n",
    "    freeze = True\n",
    "    for name, child in model.base_model.features.named_children():\n",
    "        if name == 'denseblock1':\n",
    "            freeze = False\n",
    "        if freeze:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd975a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_base_layers(model, until_layer=6):\n",
    "    \"\"\"\n",
    "    Freeze layers of ResNet-50 up to a certain stage (e.g., until_layer=6 means keep layers 0-5 frozen).\n",
    "    \"\"\"\n",
    "    child_counter = 0\n",
    "    for child in model.base_model.children():\n",
    "        if child_counter < until_layer:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        child_counter += 1\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9d81afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights for No Finding: {0: 1.0, 0.5: 1.0, 1: 1.0}\n"
     ]
    }
   ],
   "source": [
    "uncertain_weight_factor = 0.25\n",
    "class_weights = {}\n",
    "\n",
    "# Loop over each target column\n",
    "for col in target_columns:\n",
    "    # Count the occurrences of each class in the column\n",
    "    counts = filtered_train_df[col].value_counts()\n",
    "    total = len(filtered_train_df[col])\n",
    "    \n",
    "    # Calculate class weights using inverse frequency (you can also experiment with other strategies)\n",
    "    weights = {\n",
    "        0: total / (counts.get(0, 0) + 1),  # Add 1 to avoid division by zero\n",
    "        0.5: total / (counts.get(0.5, 0) + 1) * uncertain_weight_factor,\n",
    "        1: total / (counts.get(1, 0) + 1)\n",
    "    }\n",
    "    \n",
    "    # Store weights for each class\n",
    "    class_weights[col] = weights\n",
    "class_weights[target_col] = {0: 1.0, 0.5: 1.0, 1: 1.0}  ##GET RID OF THIS LINE IF DONT HAVE SAMPLER\n",
    "# Example: Print out the weights for each class\n",
    "for col in target_columns:\n",
    "    print(f\"Class weights for {col}: {class_weights[col]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7faeaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "'''def masked_MSE_loss(output, target, class_weights):\n",
    "    # Create a mask for non-NaN target values\n",
    "    mask = ~torch.isnan(target)\n",
    "    \n",
    "    # Apply the MSE loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Loop through each class and apply the class weights\n",
    "    for class_idx, col in enumerate(target_columns):\n",
    "        # Get the class values for the current class\n",
    "        class_values = target[:, class_idx]\n",
    "        \n",
    "        # Apply the class weights to each class value\n",
    "        weight = torch.tensor([class_weights[col].get(x.item(), 1) for x in class_values], dtype=torch.float32, device=output.device)\n",
    "        \n",
    "        # Apply the weight to the loss (broadcast the weight to match the loss shape)\n",
    "        loss = loss * mask  # Apply mask to exclude NaN targets\n",
    "        loss[:, class_idx] *= weight  # Apply weight per class\n",
    "    \n",
    "    # Return mean loss for valid entries\n",
    "    return loss.sum() / mask.sum()'''\n",
    "\n",
    "def masked_MSE_loss(output, target, class_weights):\n",
    "    # Compute element-wise MSE\n",
    "    loss = criterion(output, target)  # shape: [batch_size, 1]\n",
    "\n",
    "    # Get weights per target value (assume target values are 0.0, 0.5, or 1.0)\n",
    "    weights = torch.tensor([class_weights.get(float(val.item()), 1.0) for val in target.squeeze()],\n",
    "                           dtype=torch.float32,\n",
    "                           device=output.device).unsqueeze(1)  # shape: [batch_size, 1]\n",
    "\n",
    "    # Apply weights to loss\n",
    "    weighted_loss = loss * weights\n",
    "\n",
    "    return weighted_loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06234449",
   "metadata": {},
   "source": [
    "## No Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0e33376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.1394, Accuracy: 0.6345\n",
      "Validation Loss: 0.1232, Validation Accuracy: 0.5978\n",
      "Epoch [2/30], Loss: 0.1205, Accuracy: 0.6641\n",
      "Validation Loss: 0.1235, Validation Accuracy: 0.5537\n",
      "Epoch [3/30], Loss: 0.1069, Accuracy: 0.6877\n",
      "Validation Loss: 0.1263, Validation Accuracy: 0.6050\n",
      "Epoch [4/30], Loss: 0.0725, Accuracy: 0.7418\n",
      "Validation Loss: 0.1396, Validation Accuracy: 0.6186\n",
      "⛔ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "##NEW VERSION FOR FRACTURE and using for NF\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# Hyperparameters and model setup\n",
    "num_classes = 1  # Predicting 'Pleural Effusion'\n",
    "model = MultiLabelDenseNet121(num_classes=num_classes, hidden_dim = 216).to(device)\n",
    "model = freeze_base_layers_dn(model)  # Freeze layers\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = masked_MSE_loss(outputs, labels, class_weights)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_class = torch.where(outputs > 0.5, torch.tensor(1.0).to(device),\n",
    "                                      torch.where(outputs < -0.5, torch.tensor(-1.0).to(device), torch.tensor(0.0).to(device)))\n",
    "        correct += (predicted_class == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = masked_MSE_loss(outputs, labels, class_weights)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicted_class = torch.where(\n",
    "                outputs < 0.25, torch.tensor(0.0).to(device),\n",
    "                torch.where(\n",
    "                    outputs < 0.75, torch.tensor(0.5).to(device),\n",
    "                    torch.tensor(1.0).to(device)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            all_preds.append(predicted_class.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            val_correct += (predicted_class == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        #print(\"✅ Saved new best model.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "     #   print(f\"⏳ No improvement. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "\n",
    "    # Stop if patience exceeded\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e96eb637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 90703\n",
      "Sampler length: 90703\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset length: {len(train_dataset)}\")\n",
    "print(f\"Sampler length: {len(sampler)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e614f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Hyperparameters and model setup\u001b[39;00m\n\u001b[1;32m     10\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Predicting 'Pleural Effusion'\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMultiLabelResNet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m freeze_base_layers(model, until_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Freeze layers\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mMultiLabelResNet50.__init__\u001b[0;34m(self, num_classes, hidden_dim, dropout_rate)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(MultiLabelResNet50, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load pre-trained ResNet50\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResNet50_Weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMAGENET1K_V1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Modify the fully connected layer for multi-label classification\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     10\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, hidden_dim),  \u001b[38;5;66;03m# New intermediate layer. ##512 --> 256\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m    \u001b[38;5;66;03m# nn.Sigmoid()  # Sigmoid activation for multi-label classification\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torchvision/models/resnet.py:763\u001b[0m, in \u001b[0;36mresnet50\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m weights \u001b[38;5;241m=\u001b[39m ResNet50_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m--> 763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBottleneck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torchvision/models/resnet.py:301\u001b[0m, in \u001b[0;36m_resnet\u001b[0;34m(block, layers, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet(block, layers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:2573\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2566\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2567\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   2568\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2571\u001b[0m         )\n\u001b[0;32m-> 2573\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   2576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:2561\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2555\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2556\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2557\u001b[0m             k: v\n\u001b[1;32m   2558\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2559\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2560\u001b[0m         }\n\u001b[0;32m-> 2561\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:2561\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2555\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2556\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2557\u001b[0m             k: v\n\u001b[1;32m   2558\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2559\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2560\u001b[0m         }\n\u001b[0;32m-> 2561\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:2561\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2555\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2556\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2557\u001b[0m             k: v\n\u001b[1;32m   2558\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2559\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2560\u001b[0m         }\n\u001b[0;32m-> 2561\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:2544\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[1;32m   2543\u001b[0m     local_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massign_to_params_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m assign\n\u001b[0;32m-> 2544\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2548\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:2450\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2448\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2449\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2450\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2452\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "\n",
    "# Hyperparameters and model setup\n",
    "num_classes = 1  # Predicting 'Pleural Effusion'\n",
    "model = MultiLabelResNet50(num_classes=num_classes, dropout_rate = 0.6).to(device)\n",
    "model = freeze_base_layers(model, until_layer=3)  # Freeze layers\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = masked_MSE_loss(outputs, labels, class_weights)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_class = torch.where(outputs > 0.5, torch.tensor(1.0).to(device),\n",
    "                                      torch.where(outputs < -0.5, torch.tensor(-1.0).to(device), torch.tensor(0.0).to(device)))\n",
    "        correct += (predicted_class == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = masked_MSE_loss(outputs, labels, class_weights)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicted_class = torch.where(\n",
    "                outputs < 0.25, torch.tensor(0.0).to(device),\n",
    "                torch.where(\n",
    "                    outputs < 0.75, torch.tensor(0.5).to(device),\n",
    "                    torch.tensor(1.0).to(device)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            all_preds.append(predicted_class.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            val_correct += (predicted_class == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        #print(\"✅ Saved new best model.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "     #   print(f\"⏳ No improvement. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "\n",
    "    # Stop if patience exceeded\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    scheduler.step()\n",
    "#Lowest\n",
    "#Validation Loss: 0.0988, Validation Accuracy: 0.7109 (gone)\n",
    "#Validation Loss: 0.0971, Validation Accuracy: 0.7691. (nf_1)\n",
    "#Validation Loss: 0.0877 (nf_2)\n",
    "#Validation Loss: 0.0329, Validation Accuracy: 0.9083 (nf_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca91f9b",
   "metadata": {},
   "source": [
    "## With Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60abf554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with hidden_size=256, freeze_until=3, lr=0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model with hidden_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, freeze_until=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfreeze_until\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Init model and optimizer\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMultiLabelResNet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m freeze_base_layers(model, until_layer\u001b[38;5;241m=\u001b[39mfreeze_until)\n\u001b[1;32m     45\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n",
      "Cell \u001b[0;32mIn[37], line 16\u001b[0m, in \u001b[0;36mMultiLabelResNet50.__init__\u001b[0;34m(self, num_classes, hidden_size, dropout_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes, hidden_size, dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28msuper\u001b[39m(MultiLabelResNet50, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResNet50_Weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMAGENET1K_V1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     18\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, hidden_size),\n\u001b[1;32m     19\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         nn\u001b[38;5;241m.\u001b[39mTanh()  \u001b[38;5;66;03m# Tanh outputs in range [-1, 1]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     )\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torchvision/models/resnet.py:763\u001b[0m, in \u001b[0;36mresnet50\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \n\u001b[1;32m    739\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m weights \u001b[38;5;241m=\u001b[39m ResNet50_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m--> 763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBottleneck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torchvision/models/resnet.py:301\u001b[0m, in \u001b[0;36m_resnet\u001b[0;34m(block, layers, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet(block, layers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torchvision/models/_api.py:90\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/hub.py:875\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1524\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1533\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/serialization.py:2114\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   2113\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 2114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2115\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/serialization.py:2078\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2077\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 2078\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/torch/serialization.py:2031\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2024\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_offset \u001b[38;5;241m!=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_record_offset(name):\n\u001b[1;32m   2025\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2026\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a debug assert that was run as the `TORCH_SERIALIZATION_DEBUG` environment \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2027\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable was set: Incorrect offset for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstorage_offset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2028\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_file\u001b[38;5;241m.\u001b[39mget_record_offset(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2029\u001b[0m             )\n\u001b[1;32m   2030\u001b[0m     storage \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2031\u001b[0m         \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2032\u001b[0m         \u001b[38;5;241m.\u001b[39m_typed_storage()\n\u001b[1;32m   2033\u001b[0m         \u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   2034\u001b[0m     )\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameter grid\n",
    "hidden_sizes = [256, 512]\n",
    "frozen_layers = [3, 5]\n",
    "learning_rates = [5e-4, 1e-4]\n",
    "\n",
    "# Results dictionary\n",
    "results = []\n",
    "\n",
    "# Model class (same as before, now parameterized)\n",
    "class MultiLabelResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size, dropout_rate = 0.5):\n",
    "        super(MultiLabelResNet50, self).__init__()\n",
    "        self.base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.base_model.fc = nn.Sequential(\n",
    "            nn.Linear(self.base_model.fc.in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.Tanh()  # Tanh outputs in range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Freeze function\n",
    "def freeze_base_layers(model, until_layer):\n",
    "    child_counter = 0\n",
    "    for child in model.base_model.children():\n",
    "        if child_counter < until_layer:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        child_counter += 1\n",
    "    return model\n",
    "\n",
    "# Grid search loop\n",
    "for hidden_size, freeze_until, lr in itertools.product(hidden_sizes, frozen_layers, learning_rates):\n",
    "    print(f\"Training model with hidden_size={hidden_size}, freeze_until={freeze_until}, lr={lr}\")\n",
    "    \n",
    "    # Init model and optimizer\n",
    "    model = MultiLabelResNet50(num_classes=1, hidden_size=hidden_size).to(device)\n",
    "    model = freeze_base_layers(model, until_layer=freeze_until)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training for a few epochs (e.g., 5)\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = masked_MSE_loss(outputs, labels, class_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = masked_MSE_loss(outputs, labels, class_weights)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicted = torch.where(outputs > 0, torch.tensor(1.0).to(device), torch.tensor(-1.0).to(device))\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=1)\n",
    "    val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=1)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=1)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'hidden_size': hidden_size,\n",
    "        'freeze_until': freeze_until,\n",
    "        'learning_rate': lr,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_precision': val_precision,\n",
    "        'val_recall': val_recall,\n",
    "        'val_f1': val_f1\n",
    "    })\n",
    "\n",
    "# Sort and print top configs\n",
    "results.sort(key=lambda x: x['val_f1'], reverse=True)\n",
    "for r in results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6aaa7",
   "metadata": {},
   "source": [
    "## Testing Model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8477f7df",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_recall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mevaluate_model_per_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/model_pe_epoch_10.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m, in \u001b[0;36mevaluate_model_per_class\u001b[0;34m(val_loader, device, model_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m val_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     18\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mCSVDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_saved_images:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(saved_image_path):\n\u001b[0;32m---> 26\u001b[0m         image_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_image_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved tensor not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msaved_image_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:1480\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m-> 1480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_zipfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1481\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1485\u001b[0m         overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:438\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    437\u001b[0m local_header_magic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 438\u001b[0m read_bytes \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlocal_header_magic_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m read_bytes \u001b[38;5;241m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_model_per_class(val_loader, device, model_path):\n",
    "    model = MultiLabelResNet50(num_classes=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = masked_MSE_loss(outputs, labels, class_weights)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            predicted_class = torch.where(outputs > 0, torch.tensor(1.0).to(device), torch.tensor(-1.0).to(device))\n",
    "\n",
    "            all_preds.append(predicted_class.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            val_correct += (predicted_class == labels).sum().item()\n",
    "            val_total += labels.numel()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    # Flatten for sklearn metrics\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=1)\n",
    "    val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=1)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=1)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "evaluate_model_per_class(val_loader, device, \"models/model_pe_epoch_10.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b52fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution per class:\n",
      "Class 0: {np.float32(-1.0): np.int64(1829), np.float32(0.0): np.int64(169), np.float32(1.0): np.int64(252)}\n",
      "Class 1: {np.float32(-1.0): np.int64(258), np.float32(0.0): np.int64(1891), np.float32(1.0): np.int64(101)}\n",
      "Class 2: {np.float32(-1.0): np.int64(192), np.float32(0.0): np.int64(1811), np.float32(1.0): np.int64(247)}\n",
      "Class 3: {np.float32(-1.0): np.int64(56), np.float32(0.0): np.int64(1188), np.float32(1.0): np.int64(1006)}\n",
      "Class 4: {np.float32(-1.0): np.int64(34), np.float32(0.0): np.int64(2190), np.float32(1.0): np.int64(26)}\n",
      "Class 5: {np.float32(-1.0): np.int64(409), np.float32(0.0): np.int64(1045), np.float32(1.0): np.int64(796)}\n",
      "Class 6: {np.float32(-1.0): np.int64(3), np.float32(0.0): np.int64(2199), np.float32(1.0): np.int64(48)}\n",
      "Class 7: {np.float32(-1.0): np.int64(47), np.float32(0.0): np.int64(2113), np.float32(1.0): np.int64(90)}\n",
      "Class 8: {np.float32(-1.0): np.int64(50), np.float32(0.0): np.int64(1164), np.float32(1.0): np.int64(1036)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def print_label_distribution(val_loader):\n",
    "    all_labels = []\n",
    "\n",
    "    for _, labels in val_loader:\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_labels = torch.cat(all_labels, dim=0).cpu().numpy()  # shape: (num_samples, num_classes)\n",
    "\n",
    "    num_classes = all_labels.shape[1]\n",
    "\n",
    "    print(\"Label distribution per class:\")\n",
    "    for i in range(num_classes):\n",
    "        unique, counts = np.unique(all_labels[:, i], return_counts=True)\n",
    "        dist = dict(zip(unique, counts))\n",
    "        print(f\"Class {i}: {dist}\")\n",
    "print_label_distribution(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76728039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "test_dataset = CSVDataset(\n",
    "    dataframe=df_first10rows_test, \n",
    "    image_root_dir=image_root, \n",
    "    target_columns=None, \n",
    "    transform=image_transforms,  # Pass the transform\n",
    "    save_dir=test_save_dir, \n",
    "    use_saved_images=False  # Set to True if you want to load tensors from CSV\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, (images) in enumerate(test_loader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(\"Images shape:\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc75356",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e435444",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiLabelResNet50_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model_cm\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/cm_1.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m model_cm\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 9\u001b[0m model_fr \u001b[38;5;241m=\u001b[39m \u001b[43mMultiLabelResNet50_2\u001b[49m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m model_fr\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/best_fr4_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     11\u001b[0m model_fr\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiLabelResNet50_2' is not defined"
     ]
    }
   ],
   "source": [
    "model_pe = MultiLabelResNet50(num_classes=1).to(device)\n",
    "model_pe.load_state_dict(torch.load('models/pe_1.pth'))\n",
    "model_pe.eval()\n",
    "\n",
    "model_cm = MultiLabelResNet50(num_classes=1).to(device)\n",
    "model_cm.load_state_dict(torch.load('models/cm_1.pth'))\n",
    "model_cm.eval()\n",
    "\n",
    "model_fr = MultiLabelResNet50_2(num_classes=1).to(device)\n",
    "model_fr.load_state_dict(torch.load('models/best_fr4_model.pth'))\n",
    "model_fr.eval()\n",
    "\n",
    "'''model_lo = MultiLabelResNet50(num_classes=1).to(device)\n",
    "model_lo.load_state_dict(torch.load('models/best_lo1_model.pth'))\n",
    "model_lo.eval()''' ##BAAD\n",
    "\n",
    "model_ec= MultiLabelResNet50(num_classes=1).to(device)\n",
    "model_ec.load_state_dict(torch.load('models/best_ec1_model.pth'))\n",
    "model_ec.eval()\n",
    "\n",
    "model_nf = MultiLabelResNet50(num_classes=1).to(device)\n",
    "model_nf.load_state_dict(torch.load('models/nf_3.pth'))\n",
    "model_nf.eval() ##BADDDD DONT USEEE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c1bb319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelDenseNet121(\n",
       "  (base_model): DenseNet(\n",
       "    (features): Sequential(\n",
       "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (denseblock1): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition1): _Transition(\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock2): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition2): _Transition(\n",
       "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock3): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer17): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer18): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer19): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer20): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer21): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer22): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer23): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer24): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition3): _Transition(\n",
       "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock4): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=216, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=216, out_features=1, bias=True)\n",
       "      (4): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nf = MultiLabelDenseNet121(num_classes=num_classes, hidden_dim = 216).to(device)\n",
    "model_nf.load_state_dict(torch.load('models/nf_d1.pth'))\n",
    "model_nf.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbf0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_values = {\n",
    "    \"No Finding\": -0.734655,\n",
    "    \"Enlarged Cardiomediastinum\": -0.275805,\n",
    "    \"Cardiomegaly\": 0.190770,\n",
    "    \"Lung Opacity\": 0.836288,\n",
    "    \"Pneumonia\": 0.031183,\n",
    "    \"Pleural Effusion\": 0.384547,\n",
    "    \"Pleural Other\": 0.521795,\n",
    "    \"Fracture\": 0.392374,\n",
    "    \"Support Devices\": 0.888289\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8f9e97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22596 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22596/22596 [09:51<00:00, 38.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'test_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory containing the test images\n",
    "test_dir = 'input_images/test'\n",
    "\n",
    "# Columns for the prediction output\n",
    "columns = [\"Id\", \"No Finding\", \"Enlarged Cardiomediastinum\", \"Cardiomegaly\", \"Lung Opacity\", \n",
    "           \"Pneumonia\", \"Pleural Effusion\", \"Pleural Other\", \"Fracture\", \"Support Devices\"]\n",
    "\n",
    "# Average values for columns other than Pleural Effusion\n",
    "average_values = {\n",
    "    \"No Finding\": -0.734655,\n",
    "    \"Enlarged Cardiomediastinum\": -0.275805,\n",
    "    \"Cardiomegaly\": 0.190770,\n",
    "    \"Lung Opacity\": 0.836288,\n",
    "    \"Pneumonia\": 0.031183,\n",
    "    \"Pleural Other\": 0.521795,\n",
    "    \"Fracture\": 0.392374,\n",
    "    \"Support Devices\": 0.888289\n",
    "}\n",
    "\n",
    "# Batch size for processing\n",
    "batch_size = 64\n",
    "batch = []\n",
    "batch_filenames = []\n",
    "predictions = []\n",
    "\n",
    "file_list = [f for f in os.listdir(test_dir) if f.endswith(\".pt\")]\n",
    "\n",
    "for filename in tqdm(file_list):\n",
    "    image_path = os.path.join(test_dir, filename)\n",
    "    image_tensor = torch.load(image_path).to(device)\n",
    "    batch.append(image_tensor)\n",
    "    batch_filenames.append(filename.split('.')[0])\n",
    "\n",
    "    if len(batch) == batch_size or filename == file_list[-1]:\n",
    "        input_batch = torch.stack(batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #output_pe = model_pe(input_batch).cpu().numpy()\n",
    "            #output_cm = model_cm(input_batch).cpu().numpy()\n",
    "           # output_fr = model_fr(input_batch).cpu().numpy()\n",
    "           # output_lo = model_lo(input_batch).cpu().numpy()\n",
    "            #output_ec = model_ec(input_batch).cpu().numpy()\n",
    "            output_nf = model_nf(input_batch).cpu().numpy()\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "           # pe_score = output_pe[i][0] * 2 - 1  # Rescale from [0,1] to [-1,1] if needed\n",
    "           # cm_score = output_cm[i][0] * 2 - 1\n",
    "           # fr_score = output_fr[i][0] * 2 - 1\n",
    "            #lo_score = output_lo[i][0] * 2 - 1\n",
    "            #ec_score = output_ec[i][0] * 2 - 1\n",
    "            nf_score = output_nf[i][0] * 2 - 1\n",
    "\n",
    "            row = [batch_filenames[i]]\n",
    "            for col in columns[1:]:\n",
    "                '''if col == \"Pleural Effusion\":\n",
    "                    row.append(pe_score)\n",
    "                elif col == \"Cardiomegaly\":\n",
    "                    row.append(cm_score)\n",
    "                elif col == \"Fracture\":\n",
    "                    row.append(fr_score)\n",
    "               #elif col == \"Lung Opacity\":\n",
    "                #    row.append(lo_score)\n",
    "                elif col == \"Enlarged Cardiomediastinum\":\n",
    "                    row.append(ec_score)'''\n",
    "                if col == \"No Finding\":\n",
    "                    row.append(nf_score)\n",
    "                else:\n",
    "                    row.append(average_values.get(col, 0))\n",
    "\n",
    "            predictions.append(row)\n",
    "\n",
    "        batch = []\n",
    "        batch_filenames = []\n",
    "\n",
    "# Save predictions\n",
    "df_predictions = pd.DataFrame(predictions, columns=columns)\n",
    "df_predictions = df_predictions.sort_values(by=\"Id\")\n",
    "df_predictions.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'test_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d122f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 37\u001b[0m\n\u001b[1;32m     29\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_labels)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Optionally: round or filter if you're only interested in 0 vs 1 (binary task)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# mask = (all_labels != 0.5) & (all_preds != 0.5)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# all_preds = all_preds[mask]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# all_labels = all_labels[mask]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Confusion Matrix\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm, display_labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m])\n\u001b[1;32m     39\u001b[0m disp\u001b[38;5;241m.\u001b[39mplot(cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:340\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03mBy definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m(np.int64(0), np.int64(2), np.int64(1), np.int64(1))\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[0;32m--> 340\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[0;32m/central/groups/CS156b/2025/CodeMonkeys/.venv/lib64/python3.9/site-packages/sklearn/metrics/_classification.py:118\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    121\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n",
      "\u001b[0;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Make sure model is in eval mode\n",
    "model_nf.eval()\n",
    "\n",
    "# Store predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_nf(images)\n",
    "\n",
    "        # Apply your classification threshold logic\n",
    "        predicted_class = torch.where(\n",
    "            outputs < 0.25, torch.tensor(0.0).to(device),\n",
    "            torch.where(outputs < 0.75, torch.tensor(0.5).to(device), torch.tensor(1.0).to(device))\n",
    "        )\n",
    "\n",
    "        all_preds.extend(predicted_class.cpu().numpy().flatten())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Optionally: round or filter if you're only interested in 0 vs 1 (binary task)\n",
    "# mask = (all_labels != 0.5) & (all_preds != 0.5)\n",
    "# all_preds = all_preds[mask]\n",
    "# all_labels = all_labels[mask]\n",
    "\n",
    "float_to_int = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "int_preds = np.vectorize(float_to_int.get)(all_preds)\n",
    "int_labels = np.vectorize(float_to_int.get)(all_labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=[0.0, 0.5, 1.0])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0.0, 0.5, 1.0])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix for Pleural Effusion Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4685be88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f81460b23d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert [0.0, 0.5, 1.0] → [0, 1, 2]\n",
    "float_to_int = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "int_preds = np.vectorize(float_to_int.get)(all_preds)\n",
    "int_labels = np.vectorize(float_to_int.get)(all_labels)\n",
    "\n",
    "# Now generate the confusion matrix\n",
    "cm = confusion_matrix(int_labels, int_preds, labels=[0, 1, 2])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"0.0\", \"0.5\", \"1.0\"])\n",
    "disp.plot(cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f856226a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>100018</td>\n",
       "      <td>-0.734655</td>\n",
       "      <td>-0.275805</td>\n",
       "      <td>0.209412</td>\n",
       "      <td>0.836288</td>\n",
       "      <td>0.031183</td>\n",
       "      <td>-0.925029</td>\n",
       "      <td>0.521795</td>\n",
       "      <td>0.177863</td>\n",
       "      <td>0.888289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14253</th>\n",
       "      <td>100019</td>\n",
       "      <td>-0.734655</td>\n",
       "      <td>-0.275805</td>\n",
       "      <td>-0.716572</td>\n",
       "      <td>0.836288</td>\n",
       "      <td>0.031183</td>\n",
       "      <td>0.126851</td>\n",
       "      <td>0.521795</td>\n",
       "      <td>0.282742</td>\n",
       "      <td>0.888289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17256</th>\n",
       "      <td>100022</td>\n",
       "      <td>-0.734655</td>\n",
       "      <td>-0.275805</td>\n",
       "      <td>-0.716739</td>\n",
       "      <td>0.836288</td>\n",
       "      <td>0.031183</td>\n",
       "      <td>0.740513</td>\n",
       "      <td>0.521795</td>\n",
       "      <td>0.602488</td>\n",
       "      <td>0.888289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15017</th>\n",
       "      <td>100023</td>\n",
       "      <td>-0.734655</td>\n",
       "      <td>-0.275805</td>\n",
       "      <td>-0.613445</td>\n",
       "      <td>0.836288</td>\n",
       "      <td>0.031183</td>\n",
       "      <td>0.949374</td>\n",
       "      <td>0.521795</td>\n",
       "      <td>0.296630</td>\n",
       "      <td>0.888289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7126</th>\n",
       "      <td>100053</td>\n",
       "      <td>-0.734655</td>\n",
       "      <td>-0.275805</td>\n",
       "      <td>0.718571</td>\n",
       "      <td>0.836288</td>\n",
       "      <td>0.031183</td>\n",
       "      <td>-0.666675</td>\n",
       "      <td>0.521795</td>\n",
       "      <td>-0.006521</td>\n",
       "      <td>0.888289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "963    100018   -0.734655                   -0.275805      0.209412   \n",
       "14253  100019   -0.734655                   -0.275805     -0.716572   \n",
       "17256  100022   -0.734655                   -0.275805     -0.716739   \n",
       "15017  100023   -0.734655                   -0.275805     -0.613445   \n",
       "7126   100053   -0.734655                   -0.275805      0.718571   \n",
       "\n",
       "       Lung Opacity  Pneumonia  Pleural Effusion  Pleural Other  Fracture  \\\n",
       "963        0.836288   0.031183         -0.925029       0.521795  0.177863   \n",
       "14253      0.836288   0.031183          0.126851       0.521795  0.282742   \n",
       "17256      0.836288   0.031183          0.740513       0.521795  0.602488   \n",
       "15017      0.836288   0.031183          0.949374       0.521795  0.296630   \n",
       "7126       0.836288   0.031183         -0.666675       0.521795 -0.006521   \n",
       "\n",
       "       Support Devices  \n",
       "963           0.888289  \n",
       "14253         0.888289  \n",
       "17256         0.888289  \n",
       "15017         0.888289  \n",
       "7126          0.888289  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions = pd.DataFrame(predictions, columns=columns)\n",
    "df_predictions = df_predictions.sort_values(by=\"Id\", ascending=True)\n",
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff492d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/central/groups/CS156b/2025/CodeMonkeys\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
